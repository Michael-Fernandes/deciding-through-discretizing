---
title: "Final study analysis"
output: 
  html_document:
    df_print: kable
    toc: true
    self_contained: true
---

# Setup

If you have not installed RStan previously, **install it before installing any other packages below**. Rstan can be finicky to install, so we recommend carefully following the [RStan Getting Started Guide](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started). **Do not** simply issue ~~`install.packages("rstan")`~~, as this may result in a non-working RStan installation. 

Once RStan is installed, you can set up the `tidybayes` package as follows:

```{r, eval = FALSE}
install.packages("devtools")
devtools::install_github("mjskay/tidybayes")
```

Finally, install any remaining packages from the list below that you do not already have using `install.packages(c("package1", "package2", ...))`. The `import::from(packagename, function)` syntax requires the `import` package to be installed.

```{r setup, warning = FALSE, message = FALSE}
library(magrittr)
library(stringi)
library(tidyverse)
library(modelr)
library(forcats)
library(snakecase)
library(lsmeans)
library(broom)
library(lme4)
library(dotwhisker)
library(directlabels)
library(rstan)
library(brms)
library(rlang)
library(tidybayes)
library(cowplot)
library(RColorBrewer)
library(bindrcpp)
import::from(gamlss.dist, qBCT)
import::from(boot, logit)
```

```{r include = FALSE}
# cleaner ggplot theme
theme_set(
  theme_light() +
  theme(
    panel.border = element_blank(),
    axis.line = element_line(color = "grey70"),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.x = element_blank()
  )
)

# recommended code to speed up stan
# see https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```


# Load and clean data

First, let's read in the data, do some basic name and datatype cleaning:

```{r}
df = read.csv("data/final_trials.csv") %>%
  as_tibble()

head(df)
```

The set of conditions looks like this:

```{r}
df %>%
  group_by(vis, arrival) %>%
  summarise(n(), length(unique(participant)))
```

We will add `trial_normalized`, which will go from -0.5 (the first trial) to 0.5 (the last trial) to help model convergence.

```{r}
max_trial = 39
stopifnot(max(df$trial) == max_trial)

df %<>%
  mutate(
    trial_normalized = ((trial - max_trial) / max_trial) + 0.5
  )
```

We will also add the intervals that participants would have been shown in the text conditions:

```{r}
df %<>%
  mutate(
    text_interval = round(gamlss.dist::qBCT(1 - .85, mu, sigma, nu, tau) - 15),
    text60_interval = round(gamlss.dist::qBCT(1 - .60, mu, sigma, nu, tau) - 15),
    text99_interval = round(gamlss.dist::qBCT(1 - .99, mu, sigma, nu, tau) - 15)
  )
```

We'll also exclude scenario 4 for now, since it would need to be analyzed separately (because it is binary choice) and because we did not pre-register a model for it:

```{r}
df %<>%
  filter(scenario != "s4") %>%
  mutate(scenario = factor(scenario))
```

Finally, for the beta regression models, we will need `expected_over_optimal` to be guaranteed to be between 0 and 1 (exclusive) --- currently it is between 0 and 1 (inclusive). So we'll create an "adjusted" normalized response that is guaranteed to be between 0 and 1 (exclusive). This just adjusts values that would be 1 downward slightly:

```{r}
df %<>%
  mutate(
    expected_over_optimal_adjusted = ifelse(expected_payoff == optimal_payoff, 
      optimal_payoff / (optimal_payoff + 1),
      expected_payoff / optimal_payoff
    )
  )
```

This transformation does not substantially change responses, but does allow us to use the logit transformation on them (or use the beta distribution to model them, as we will see shortly). You can see that the distributions are essentially identical (black is original, red is adjusted):

```{r}
df %>%
  filter(trial > 35) %>% #just look at the last couple of trials
  ggplot(aes(x = expected_over_optimal)) +
  stat_density() +
  stat_density(aes(x = expected_over_optimal_adjusted), fill = NA, color = "red") +
  facet_wrap(~ vis)
```

These plots show distributions of responses in each condition in the last 5 trials. This also suggests that beta regression might be an appropriate choice, since the conditional distributions are beta-like.

An excerpt from the cleaned dataset:

```{r}
head(df)
```



# Naive learning curves

## By vis

Let's start with a high-level view (and a somewhat poor model: linear without accounting for participant or scenario) to see how people improve over the course of the trials in the different conditions:

```{r, fig.height = 10, fig.width = 7}
df %>%
  ggplot(aes(x = trial, y = expected_over_optimal)) +
  stat_summary(fun.data = mean_se) +
  geom_hline(yintercept = 1) +
  stat_smooth(method = lm) +
  facet_wrap(~ vis)
```

## By arrival

The other question is if providing the arrival information at the right makes a difference.

```{r, fig.height = 6, fig.width = 8}
df %>%
  ggplot(aes(x = trial, y = expected_over_optimal, color = arrival)) +
  stat_summary(fun.data = mean_se, position = position_dodge(width = .5)) +
  geom_hline(yintercept = 1) +
  stat_smooth(method = loess) 
```

It doesn't seem to make a huge difference, so for simplicity we will pool the arrival variants. We saw something similar in the pilot analysis, and a pooled model for arrival is what we [pre-registered](http://aspredicted.org/blind.php?x=g2yb2f).


# Bayesian beta regression

One of the displeasing aspects of applying a linear model here is that it won't work well for prediction --- the data is skewed and bounded above at 1. Since normalized responses will always be between 0 and 1, a logit transformation might help here. This would also ensure that learning curves can approach -- but never pass --- optimal, which is a property we should expect them to have.

## Model

To build a model for these data, we'll use a beta regression, which is bounded on (0,1), and uses a logit link.

### Priors

These are the priors we can set:

```{r}
get_prior(bf(
    expected_over_optimal_adjusted ~ trial_normalized + (trial_normalized|participant) + (1|scenario), 
    phi ~ vis*trial_normalized
  ),
  data = df, family = Beta)
```

We'll set weakly-informed priors for the various classes of coefficients, as per [our pre-registration](http://aspredicted.org/blind.php?x=g2yb2f):

```{r}
pr_beta = c(
  prior(normal(0, 1), class = b),
  # these prior intercepts are wide and cover 0 (50% on the logit scale), but
  # also assume some likely better-than-50% performance on average --- this
  # was chosen to aid convergence during the pilot, but does not have a strong
  # impact on final estimates.
  prior(normal(2, 2), class = Intercept),
  prior(normal(2, 2), class = Intercept, dpar = phi),
  prior(normal(0, 1), class = b, dpar = phi),
  prior(student_t(3, 0, 1), class = sd)
)
```

### Fit model

Let's fit the model:

```{r, eval = FALSE}
warmup = 2000
iter = warmup + 2000
thin = 2

mbeta = brm(bf(
    expected_over_optimal_adjusted ~ vis*trial_normalized + (trial_normalized|participant) + (1|scenario), 
    phi ~ vis*trial_normalized),
  data = df, prior = pr_beta, 
  control = list(adapt_delta = 0.9995, max_treedepth = 15, stepsize = 0.005),
  warmup = warmup, iter = iter, thin = thin,
  family = Beta
  )
```

Note that the above model takes a long time to run (about 24 hours), primarily because we are estimating a random effect for a factor with only three groups (scenario)---See Gelman (2006), *Prior distributions for variance parameters in hierarchical models* for more discussion of this issue. In retrospect we should have pre-registered a zero-avoiding prior for variance hyperpriors (with a non-zero-centered tighter truncated t prior it only takes an hour or two to fit and yields relatively similar estimates), but we pre-registered the above priors, so we'll stick to them. To save time we'll just load the fitted model from disk:

```{r}
mbeta = read_rds("models/final_model.rds")
```

Some model diagnostics (note the funnel shape on `sd_scenario__Intercept`, this is why our model takes awhile to fit):

```{r}
pairs(mbeta$fit, pars = c("b_Intercept", "b_trial_normalized", "sd_participant__Intercept",  "sd_participant__trial_normalized", 
  "sd_scenario__Intercept",  
  "cor_participant__Intercept__trial_normalized", "b_phi_Intercept", "b_phi_trial_normalized"))
```


## Posterior prediction

We'll start with posterior predictions. First we'll decide on a display order for consistency across charts:

```{r}
vis_display_order = c("dot50", "cdf", "dot20", "text99", "text60", "pdfinterval", "pdf", "interval", "text", "none")
```

Then we'll generate posterior predictions:

```{r}
pred_beta = 
  df %>%
  data_grid(
    vis,
    trial_normalized = seq_range(trial_normalized, n = 20)
  ) %>%
  add_predicted_samples(mbeta, re_formula = NULL, allow_new_levels = TRUE) %>%    
  mean_qi(.prob = c(.95, .8, .5))
```

And plot them, along with data (note that some conditions, notably `pdf`, had more data collected):

```{r fig.height = 3.5, fig.width = 9}
# shared properties of posterior prediction and fit line plots
fit_line_plot_settings = list(
  scale_x_continuous(breaks = seq(-0.5, 0.5, length.out = 40), labels = c("1", rep("", 38), "40")),
  coord_cartesian(expand = FALSE),
  xlab("Trial"),
  theme(
    panel.grid = element_blank(), panel.spacing.x = unit(5, "points"),
    strip.background = element_blank(), strip.text = element_text(hjust = 0.5, color = "black"),
    axis.title.x = element_text(hjust = 0)
  ))

post_pred_plot = pred_beta %>%
  ungroup() %>%
  mutate(vis = fct_relevel(vis, vis_display_order)) %>%
  ggplot(aes(x = trial_normalized)) +
  geom_lineribbon(aes(y = pred), data = pred_beta) +
  geom_hline(yintercept = 1) +
  scale_fill_brewer(guide = guide_legend(reverse = TRUE)) +
  geom_hline(yintercept = seq(.4, .95, by=.1), color="gray75", alpha = 0.5) +
  fit_line_plot_settings + 
  facet_grid(. ~ vis) +
  ylab("Performance / Optimal strategy")
post_pred_plot +
  geom_point(aes(y = expected_over_optimal), alpha = 0.05, data = df) 
```

Above, the red line is the predicted median, and the blue bands are predictive intervals for the data. We can see that most conditions get better both in terms of bias and variance over time: people get closer to optimal, and variance in performance decreases (people get more consistent). However, there are some differences, and it is hard to tell how reliable those differences are just by looking at predictions. So, let's look at posteriors for the mean and precision of estimates according to the model.


## Model-based learning curves

First we'll generate samples of fit lines for mu and phi. We will use these to plot fit lines and to generate estimates for performance in the last trial. These estimates will be for the "average" scenario and "average" person:


```{r}
fit_lines = df %>%
  data_grid(
    vis,
    trial_normalized = seq_range(trial_normalized, n = 20)
  ) %>%
  add_fitted_samples(mbeta, re_formula = NA, var = "mu") %>%
  ungroup() %>%
  mutate(vis = fct_relevel(vis, vis_display_order))
```

The estimates of $\phi$ (`phi`, the precision parameter of the beta distribution) are a little hard to interpret, so instead we'll derive a posterior distribution for standard deviation. We can use the fact that the standard deviation $\sigma$ of a Beta distribution is:

$$
\sigma = \sqrt{\frac{\mu (1 - \mu)}{(1 + \phi)}}
$$

Thus we can transform samples from the distribution of $\mu$ (`mu`) and $\phi$ (`phi`) into samples from the distribution of $\sigma$ (`sd`):

```{r}
fit_lines %<>%
  mutate(sd = sqrt(mu * (1 - mu) / (1 + phi)))
```


### Trends in mean

Estimates of the mean for the "average" scenario and "average" person:

```{r, fig.height = 3, fig.width = 8}
scale_fill_fit_lines = scale_fill_manual(
  values = RColorBrewer::brewer.pal(4, "Greys")[-1], guide = guide_legend(reverse = TRUE)
)

mu_lines_plot = fit_lines %>%
  ggplot(aes(x = trial_normalized, y = mu)) +
  stat_lineribbon(.prob = c(.95, .8, .5)) +
  geom_hline(yintercept = seq(.8, 1, by=.05), color="gray75", alpha = 0.5) +
  facet_grid(. ~ vis) +
  scale_fill_fit_lines +
  fit_line_plot_settings
mu_lines_plot
```

### Trends in standard deviation

Estimates of the standard deviation for the "average" scenario and "average" person:

```{r, fig.height = 3, fig.width = 8}
sd_lines_plot = fit_lines %>%
  ggplot(aes(x = trial_normalized, y = sd)) +
  stat_lineribbon(.prob = c(.95, .8, .5)) +
  geom_hline(yintercept = seq(0, .16, by=.04), color="gray75", alpha = 0.5) +
  facet_grid(. ~ vis) +
  scale_fill_fit_lines +
  fit_line_plot_settings
sd_lines_plot
```

```{r, fig.width = 8, fig.height = 8, eval = FALSE, include = FALSE}
#combined plot (don't include in md output)
lines_plots = plot_grid(
  post_pred_plot +
    xlab(NULL) +
    scale_x_continuous(breaks = NULL) +
    scale_y_continuous(breaks = seq(.4, 1.0, by = .1), 
      labels = formatC(seq(.4, 1.0, by = .1), format = "f", digits = 2)) +
    theme(panel.border = element_rect(color = "gray75", fill = NA)),
  mu_lines_plot +
    xlab(NULL)+
    theme(strip.background = element_blank(), strip.text = element_blank()) +
    scale_x_continuous(breaks = NULL) +
    theme(panel.border = element_rect(color = "gray75", fill = NA)),
  sd_lines_plot +
    theme(strip.background = element_blank(), strip.text = element_blank()) +
    theme(panel.border = element_rect(color = "gray75", fill = NA)),
  align = "h",
  ncol = 1,
  axis = "l"
)
lines_plots

pdf(file = "lines_plots.pdf", useDingbats = FALSE, width = 8, height = 8)
lines_plots
dev.off()
```



## Performance on last trial

The above trend lines let us see more clearly how performance evolved as people learned. But how did people perform in the last trial? Let's get estimates of the mean and precision for the "average" person in the "average" scenario on the last trial:

```{r}
last_trial = df %>%
  data_grid(
    vis,
    trial_normalized = 0.5  # because we normalized trial to be from -0.5 to 0.5
  ) %>%
  add_fitted_samples(mbeta, re_formula = NA, var = "mu") %>%
  ungroup() %>%
  mutate(vis = fct_rev(fct_relevel(vis, vis_display_order))) %>%
  mutate(sd = sqrt(mu * (1 - mu) / (1 + phi)))
```

### Mean

Conditional means (for "average" person on "average" scenario) on last trial:

```{r}
plot_means = last_trial %>%
  ggplot(aes(y = vis, x = mu)) +
  geom_halfeyeh(fun.data = median_qih, fatten.point = 1.3) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "black") +
  coord_cartesian(xlim = c(0.85, 1.0), ylim = c(1, 10.5)) 
plot_means
```

Let's look at differences between each condition and the control (no uncertainty) and best-performing (dot50) conditions.

Difference to control:

```{r}
plot_means_vs_text = last_trial %>%
  mutate(vis = fct_relevel(vis, "none")) %>%
  compare_levels(mu, by = vis, comparison = control) %>%
  mutate(vis = factor(vis, levels = c("none - none", levels(vis)))) %>%
  bind_rows(data_frame(vis = factor("none - none", levels = levels(.$vis)), mu = 0)) %>%
  ggplot(aes(y = vis, x = mu)) +
  geom_halfeyeh(fun.data = median_qih, fatten.point = 1.3) +
  geom_vline(xintercept = 0, color = "black", linetype = "dashed") +
  coord_cartesian(xlim = c(-0.05, 0.09), ylim = c(1, 10.5))
plot_means_vs_text
```

Difference to dot50:

```{r}
plot_means_vs_dot50 = last_trial %>%
  mutate(vis = fct_relevel(vis, "dot50")) %>%
  compare_levels(mu, by = vis, comparison = control) %>%
  mutate(vis = factor(vis, levels = c(levels(vis), "dot50 - dot50"))) %>%
  bind_rows(data_frame(vis = factor("dot50 - dot50", levels = levels(.$vis)), mu = 0)) %>%
  ggplot(aes(y = vis, x = mu)) +
  geom_vline(xintercept = 0, color = "black", linetype = "dashed") +
  geom_halfeyeh(fun.data = median_qih, fatten.point = 1.3) +
  coord_cartesian(xlim = c(-0.11, 0.01), ylim = c(1, 10.5))
plot_means_vs_dot50
```

```{r, fig.width = 8, fig.height = 3, eval = FALSE, include = FALSE}
#combined plot (not for md)
plot_grid(
  plot_means +
    xlab("mean,\nlast trial")+
    ylab(NULL), 
  plot_means_vs_text + 
    scale_y_discrete(labels = NULL) +
    ylab(NULL) +
    xlab("mean - control mean,\nlast trial") +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank()),
  plot_means_vs_dot50 + 
    scale_y_discrete(labels = NULL) +
    ylab(NULL) +
    xlab("mean - dot50 mean,\nlast trial") +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank()),
  align = "h",
  ncol = 3,
  rel_widths = c(1.3, 1, 1),
  axis = "b"
)
```

### Standard deviation

Conditional standard deviation (for "average" person on "average" scenario) on last trial:

```{r}
plot_prec = last_trial %>%
  ggplot(aes(y = vis, x = sd)) +
  geom_halfeyeh(fun.data = median_qih, fatten.point = 1.3) +
  geom_vline(xintercept = 0, color = "black", linetype = "dashed") +
  coord_cartesian(xlim = c(0, .12), ylim = c(1, 10.5))
plot_prec
```

Difference to control:

```{r}
plot_prec_vs_text = last_trial %>%
  mutate(vis = fct_relevel(vis, "none")) %>%
  compare_levels(sd, by = vis, comparison = control) %>%
  mutate(vis = factor(vis, levels = c("none - none", levels(vis)))) %>%
  bind_rows(data_frame(vis = factor("none - none", levels = levels(.$vis)), sd = 0)) %>%
  ggplot(aes(y = vis, x = sd)) +
  geom_halfeyeh(fun.data = median_qih, fatten.point = 1.3) +
  geom_vline(xintercept = 0, color = "black", linetype = "dashed") +
  coord_cartesian(ylim = c(1, 10.5), xlim = c(-.06, 0.05))
plot_prec_vs_text
```

Difference to dot50:

```{r}
plot_prec_vs_dot50 = last_trial %>%
  mutate(vis = fct_relevel(vis, "dot50")) %>%
  compare_levels(sd, by = vis, comparison = control) %>%
  mutate(vis = factor(vis, levels = c(levels(vis), "dot50 - dot50"))) %>%
  bind_rows(data_frame(vis = factor("dot50 - dot50", levels = levels(.$vis)), sd = 0)) %>%
  ggplot(aes(y = vis, x = sd)) +
  geom_vline(xintercept = 0, color = "black", linetype = "dashed") +
  geom_halfeyeh(fun.data = median_qih, fatten.point = 1.3) +
  coord_cartesian(xlim = c(0, .085), ylim = c(1, 10.5))
plot_prec_vs_dot50
```

```{r, fig.width = 8, fig.height = 2.5, eval = FALSE, include = FALSE}
#altogether (not for md)
plot_grid(
  plot_prec +
    xlab("sd, last trial")+
    ylab(NULL), 
  plot_prec_vs_text + 
    scale_y_discrete(labels = NULL) +
    ylab(NULL) +
    xlab("sd - text sd, last trial") +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank()),
  plot_prec_vs_dot50 + 
    scale_y_discrete(labels = NULL) +
    ylab(NULL) +
    xlab("sd - dot50 sd, last trial") +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank()),
  align = "h",
  ncol = 3,
  rel_widths = c(1.3, 1, 1),
  axis = "b"
)
```



```{r, fig.width = 8, fig.height = 2.5, eval = FALSE, include = FALSE}
# Mean and SD, comparisons only, not for md
last_trial_plots = plot_grid(
  plot_means_vs_text +
    ylab(NULL) +
    xlab("mean - control mean, last trial") +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank()),
  plot_means_vs_dot50 +
    scale_y_discrete(labels = NULL) +
    ylab(NULL) +
    xlab("mean - dot50 mean, last trial") +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank()),
  plot_prec_vs_text + 
    scale_y_discrete(labels = NULL) +
    ylab(NULL) +
    xlab("sd - text sd, last trial") +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank()),
  plot_prec_vs_dot50 + 
    scale_y_discrete(labels = NULL) +
    ylab(NULL) +
    xlab("sd - dot50 sd, last trial") +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank()),
  align = "h",
  ncol = 4,
  rel_widths = c(1.5, 1, 1, 1),
  axis = "b"
)
last_trial_plots

pdf(file = "last_trial_plots.pdf", useDingbats = FALSE, width = 8, height = 2.5)
last_trial_plots
dev.off()
```
